{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-22T19:03:48.608867Z",
     "start_time": "2025-12-22T19:03:48.580422Z"
    }
   },
   "source": [
    "# It is copied from here - https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/11_qwen3/standalone-qwen3.ipynb\n",
    "\n",
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"huggingface_hub\",  # to download pretrained weights\n",
    "    \"tokenizers\",       # to implement the tokenizer\n",
    "    \"torch\",            # to implement the model\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface_hub version: 1.2.3\n",
      "tokenizers version: 0.22.1\n",
      "torch version: 2.9.1\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T19:07:50.835958Z",
     "start_time": "2025-12-22T19:07:50.824291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Select which model to use via the following flag; only one can be True\n",
    "\n",
    "USE_BASE_MODEL = False\n",
    "USE_REASONING_MODEL = True\n",
    "USE_INSTRUCT_MODEL = False\n",
    "\n",
    "if (USE_BASE_MODEL + USE_REASONING_MODEL\n",
    "    + USE_INSTRUCT_MODEL) != 1:\n",
    "    raise AttributeError(\"Only one of the options above can be True.\")"
   ],
   "id": "8eb12627fffe6748",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T19:21:57.951735Z",
     "start_time": "2025-12-22T19:21:57.941371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.fc2 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.fc3 = nn.Linear(cfg[\"hidden_dim\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_fc1 = self.fc1(x)\n",
    "        x_fc2 = self.fc2(x)\n",
    "        x = nn.functional.silu(x_fc1) * x_fc2\n",
    "        return self.fc3(x)\n"
   ],
   "id": "b8b4a67f730fc611",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T19:21:59.171089Z",
     "start_time": "2025-12-22T19:21:59.158766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, emb_dim, eps=1e-6, bias=False, qwen3_compatible=True):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.qwen3_compatible = qwen3_compatible\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim)) if bias else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_type = x.dtype\n",
    "        if self.qwen3_compatible:\n",
    "            x = x.to(torch.float32)\n",
    "        variance = x.pow(2).mean(dim=-1, keepdim=True)\n",
    "        norm_x = x * torch.rsqrt(variance + self.eps)\n",
    "        norm_x = norm_x * self.scale\n",
    "        if self.shift is not None:\n",
    "            norm_x += self.shift\n",
    "        return norm_x.to(input_type)\n"
   ],
   "id": "5923e8ccc19d471b",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T19:24:52.145295Z",
     "start_time": "2025-12-22T19:24:52.132475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_rope_params(head_dim, theta_base=10_000, context_length=4096, dtype=torch.float32):\n",
    "    assert head_dim % 2 == 0, \"Embedding dimension must be even\"\n",
    "    inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2, dtype=dtype)[: (head_dim // 2)].float() / head_dim))\n",
    "    # Generate position indices\n",
    "    positions = torch.arange(context_length, dtype=dtype)\n",
    "\n",
    "    # Compute the angles\n",
    "    angles = positions.unsqueeze(1) * inv_freq.unsqueeze(0)  # Shape: (context_length, head_dim // 2)\n",
    "\n",
    "    # Expand angles to match the head_dim\n",
    "    angles = torch.cat([angles, angles], dim=1)  # Shape: (context_length, head_dim)\n",
    "\n",
    "    # Precompute sine and cosine\n",
    "    cos = torch.cos(angles)\n",
    "    sin = torch.sin(angles)\n",
    "\n",
    "    return cos, sin"
   ],
   "id": "1cb5e548d21676a9",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T19:52:45.417563Z",
     "start_time": "2025-12-22T19:52:45.387733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def apply_rope(x, cos, sin):\n",
    "    # x: bsz, num_heads, seq_len, head_dim\n",
    "    bsz, num_heads, seq_len, head_dim = x.shape\n",
    "    assert head_dim % 2 == 0, \"Head dimension must be even\"\n",
    "\n",
    "    x1 = x[..., :head_dim//2]\n",
    "    x2 = x[..., head_dim//2:]\n",
    "\n",
    "    # Adjust sin and cos shapes\n",
    "    cos = cos[:seq_len, :].unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, seq_len, head_dim)\n",
    "    sin = sin[:seq_len, :].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Apply the rotary transformation\n",
    "    rotated = torch.cat((-x2, x1), dim=-1)\n",
    "    x_rotated = (x * cos) + (rotated * sin)\n",
    "\n",
    "    # It's ok to use lower-precision after applying cos and sin rotation\n",
    "    return x_rotated.to(dtype=x.dtype)\n",
    "\n"
   ],
   "id": "d9798fb41276da98",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:12:22.417853Z",
     "start_time": "2025-12-22T20:12:22.405829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            d_in,\n",
    "            num_heads,\n",
    "            num_kv_groups,\n",
    "            head_dim=None,\n",
    "            qk_norm=False,\n",
    "            dtype=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert num_heads % num_kv_groups == 0, \"num_heads must be divisible by num_kv_groups\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.num_kv_groups = num_kv_groups\n",
    "        self.group_size = num_heads // num_kv_groups\n",
    "\n",
    "        if head_dim is None:\n",
    "            assert d_in % num_heads == 0, \"`d_in` must be divisible by `num_heads` if `head_dim` is not set\"\n",
    "            head_dim = d_in // num_heads\n",
    "\n",
    "        self.head_dim = head_dim\n",
    "        self.d_out = num_heads * head_dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, self.d_out, bias=False, dtype=dtype)\n",
    "        self.W_key = nn.Linear(d_in, num_kv_groups * head_dim, bias=False, dtype=dtype)\n",
    "        self.W_value = nn.Linear(d_in, num_kv_groups * head_dim, bias=False, dtype=dtype)\n",
    "\n",
    "        self.out_proj = nn.Linear(self.d_out, d_in, bias=False, dtype=dtype)\n",
    "\n",
    "        if qk_norm:\n",
    "            self.q_norm = RMSNorm(head_dim, eps=1e-6)\n",
    "            self.k_norm = RMSNorm(head_dim, eps=1e-6)\n",
    "        else:\n",
    "            self.q_norm = self.k_norm = None\n",
    "\n",
    "    def forward(self, x, mask, cos, sin):\n",
    "        b, num_tokens, _ = x.shape\n",
    "\n",
    "        # Apply projections\n",
    "        queries = self.W_query(x)  # (b, num_tokens, num_heads * head_dim)\n",
    "        keys = self.W_key(x)       # (b, num_tokens, num_kv_groups * head_dim)\n",
    "        values = self.W_value(x)   # (b, num_tokens, num_kv_groups * head_dim)\n",
    "\n",
    "        #reshape\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2) # (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2) # (b, num_kv_groups, num_tokens, head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2) # (b, num_kv_groups, num_tokens, head_dim)\n",
    "\n",
    "        # Optional normalization\n",
    "        if self.q_norm:\n",
    "            queries = self.q_norm(queries)\n",
    "        if self.k_norm:\n",
    "            keys = self.k_norm(keys)\n",
    "\n",
    "        # Apply RoPE\n",
    "        queries = apply_rope(queries, cos, sin)\n",
    "        keys = apply_rope(keys, cos, sin)\n",
    "\n",
    "        # Expand K and V to match number of heads\n",
    "        keys = keys.repeat_interleave(self.group_size, dim=1)\n",
    "        values = values.repeat_interleave(self.group_size, dim=1)\n",
    "\n",
    "        # Attention\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        attn_scores = attn_scores.masked_fill(mask, -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / self.head_dim**0.5, dim=-1)\n",
    "\n",
    "        context = (attn_weights @ values).transpose(1, 2).reshape(b, num_tokens, self.d_out)\n",
    "        return self.out_proj(context)\n"
   ],
   "id": "31ea73ffb0e0f87f",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:12:48.742292Z",
     "start_time": "2025-12-22T20:12:48.727780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = GroupedQueryAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            head_dim=cfg[\"head_dim\"],\n",
    "            num_kv_groups=cfg[\"n_kv_groups\"],\n",
    "            qk_norm=cfg[\"qk_norm\"],\n",
    "            dtype=cfg[\"dtype\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
    "        self.norm2 = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
    "\n",
    "    def forward(self, x, mask, cos, sin):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x, mask, cos, sin)\n",
    "        x = x + shortcut\n",
    "        #shortcut connection for feedforward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x\n"
   ],
   "id": "7ba0ae50e35b3719",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:12:49.448453Z",
     "start_time": "2025-12-22T20:12:49.433789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Qwen3Model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        # Main model parameters\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"])\n",
    "        self.trf_blocks = nn.ModuleList(  # ModuleList since Sequential can only accept one input, and we need `x, mask, cos, sin`\n",
    "            [TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = RMSNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False, dtype=cfg[\"dtype\"])\n",
    "\n",
    "        # Reusuable utilities\n",
    "        if cfg[\"head_dim\"] is None:\n",
    "            head_dim = cfg[\"emb_dim\"] // cfg[\"n_heads\"]\n",
    "        else:\n",
    "            head_dim = cfg[\"head_dim\"]\n",
    "        cos, sin = compute_rope_params(\n",
    "            head_dim=head_dim,\n",
    "            theta_base=cfg[\"rope_base\"],\n",
    "            context_length=cfg[\"context_length\"]\n",
    "        )\n",
    "        self.register_buffer(\"cos\", cos, persistent=False)\n",
    "        self.register_buffer(\"sin\", sin, persistent=False)\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def forward(self, x_idx):\n",
    "        tok_emb = self.tok_emb(x_idx)\n",
    "        x = tok_emb\n",
    "        num_tokens = x.shape[1]\n",
    "        mask = torch.triu(torch.ones(num_tokens, num_tokens, device=x.device, dtype=torch.bool), diagonal=1)\n",
    "        for block in self.trf_blocks:\n",
    "            x = block(x, mask, self.cos, self.sin)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x.to(self.cfg[\"dtype\"]))\n",
    "        return logits\n"
   ],
   "id": "453142862748e84e",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:12:50.101376Z",
     "start_time": "2025-12-22T20:12:50.091374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CHOOSE_MODEL = \"0.6B\"\n",
    "\n",
    "if CHOOSE_MODEL == \"0.6B\":\n",
    "    QWEN3_CONFIG = {\n",
    "        \"vocab_size\": 151_936,           # Vocabulary size\n",
    "        \"context_length\": 40_960,        # Context length that was used to train the model\n",
    "        \"emb_dim\": 1024,                 # Embedding dimension\n",
    "        \"n_heads\": 16,                   # Number of attention heads\n",
    "        \"n_layers\": 28,                  # Number of layers\n",
    "        \"hidden_dim\": 3072,              # Size of the intermediate dimension in FeedForward\n",
    "        \"head_dim\": 128,                 # Size of the heads in GQA\n",
    "        \"qk_norm\": True,                 # Whether to normalize queries and keys in GQA\n",
    "        \"n_kv_groups\": 8,                # Key-Value groups for grouped-query attention\n",
    "        \"rope_base\": 1_000_000.0,        # The base in RoPE's \"theta\"\n",
    "        \"dtype\": torch.bfloat16,         # Lower-precision dtype to reduce memory usage\n",
    "    }\n",
    "\n",
    "elif CHOOSE_MODEL == \"1.7B\":\n",
    "    QWEN3_CONFIG = {\n",
    "        \"vocab_size\": 151_936,\n",
    "        \"context_length\": 40_960,\n",
    "        \"emb_dim\": 2048,                 # 2x larger than above\n",
    "        \"n_heads\": 16,\n",
    "        \"n_layers\": 28,\n",
    "        \"hidden_dim\": 6144,              # 2x larger than above\n",
    "        \"head_dim\": 128,\n",
    "        \"qk_norm\": True,\n",
    "        \"n_kv_groups\": 8,\n",
    "        \"rope_base\": 1_000_000.0,\n",
    "        \"dtype\": torch.bfloat16,\n",
    "    }\n",
    "\n",
    "elif CHOOSE_MODEL == \"4B\":\n",
    "    QWEN3_CONFIG = {\n",
    "        \"vocab_size\": 151_936,\n",
    "        \"context_length\": 40_960,\n",
    "        \"emb_dim\": 2560,                 # 25% larger than above\n",
    "        \"n_heads\": 32,                   # 2x larger than above\n",
    "        \"n_layers\": 36,                  # 29% larger than above\n",
    "        \"hidden_dim\": 9728,              # ~3x larger than above\n",
    "        \"head_dim\": 128,\n",
    "        \"qk_norm\": True,\n",
    "        \"n_kv_groups\": 8,\n",
    "        \"rope_base\": 1_000_000.0,\n",
    "        \"dtype\": torch.bfloat16,\n",
    "    }\n",
    "\n",
    "elif CHOOSE_MODEL == \"8B\":\n",
    "    QWEN3_CONFIG = {\n",
    "        \"vocab_size\": 151_936,\n",
    "        \"context_length\": 40_960,\n",
    "        \"emb_dim\": 4096,                 # 60% larger than above\n",
    "        \"n_heads\": 32,\n",
    "        \"n_layers\": 36,                  # 26% larger than above\n",
    "        \"hidden_dim\": 12288,\n",
    "        \"head_dim\": 128,\n",
    "        \"qk_norm\": True,\n",
    "        \"n_kv_groups\": 8,\n",
    "        \"rope_base\": 1_000_000.0,\n",
    "        \"dtype\": torch.bfloat16,\n",
    "    }\n",
    "\n",
    "elif CHOOSE_MODEL == \"14B\":\n",
    "    QWEN3_CONFIG = {\n",
    "        \"vocab_size\": 151_936,\n",
    "        \"context_length\": 40_960,\n",
    "        \"emb_dim\": 5120,                 # 25% larger than above\n",
    "        \"n_heads\": 40,                   # 25% larger than above\n",
    "        \"n_layers\": 40,                  # 11% larger than above\n",
    "        \"hidden_dim\": 17408,             # 42% larger than above\n",
    "        \"head_dim\": 128,\n",
    "        \"qk_norm\": True,\n",
    "        \"n_kv_groups\": 8,\n",
    "        \"rope_base\": 1_000_000.0,\n",
    "        \"dtype\": torch.bfloat16,\n",
    "    }\n",
    "\n",
    "elif CHOOSE_MODEL == \"32B\":\n",
    "    QWEN3_CONFIG = {\n",
    "        \"vocab_size\": 151_936,\n",
    "        \"context_length\": 40_960,\n",
    "        \"emb_dim\": 5120,\n",
    "        \"n_heads\": 64,                   # 60% larger than above\n",
    "        \"n_layers\": 64,                  # 60% larger than above\n",
    "        \"hidden_dim\": 25600,             # 47% larger than above\n",
    "        \"head_dim\": 128,\n",
    "        \"qk_norm\": True,\n",
    "        \"n_kv_groups\": 8,\n",
    "        \"rope_base\": 1_000_000.0,\n",
    "        \"dtype\": torch.bfloat16,\n",
    "    }\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"{CHOOSE_MODEL} is not supported.\")"
   ],
   "id": "339ecda29fbc638d",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:12:52.860457Z",
     "start_time": "2025-12-22T20:12:50.636341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "model = Qwen3Model(QWEN3_CONFIG)"
   ],
   "id": "16666044226e948b",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:12:56.174301Z",
     "start_time": "2025-12-22T20:12:56.146544Z"
    }
   },
   "cell_type": "code",
   "source": "model",
   "id": "c7d183155c9db416",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3Model(\n",
       "  (tok_emb): Embedding(151936, 1024)\n",
       "  (trf_blocks): ModuleList(\n",
       "    (0-27): 28 x TransformerBlock(\n",
       "      (att): GroupedQueryAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        (q_norm): RMSNorm()\n",
       "        (k_norm): RMSNorm()\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (fc2): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (fc3): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "      )\n",
       "      (norm1): RMSNorm()\n",
       "      (norm2): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (final_norm): RMSNorm()\n",
       "  (out_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:17:57.966301Z",
     "start_time": "2025-12-22T20:17:57.950867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "xx = torch.tensor([1,2,3]).unsqueeze(0)\n",
    "xx.shape"
   ],
   "id": "c49846d2d1c4fa1a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:18:30.117275Z",
     "start_time": "2025-12-22T20:18:30.036964Z"
    }
   },
   "cell_type": "code",
   "source": "model(xx)",
   "id": "a771c31b8807c92e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2256, -0.0164, -0.7070,  ...,  0.4414,  0.1245,  1.0703],\n",
       "         [-0.6602,  0.5352, -0.0718,  ..., -0.0737,  0.5391,  0.3086],\n",
       "         [-0.4785, -0.1562,  0.1045,  ..., -0.2324,  0.2354,  0.6328]]],\n",
       "       dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:19:37.039104Z",
     "start_time": "2025-12-22T20:19:37.012107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ],
   "id": "eab334b231b45c97",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 751632384\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:20:42.032918Z",
     "start_time": "2025-12-22T20:20:42.010661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_params_normalized = total_params - model.tok_emb.weight.numel()\n",
    "print(f\"\\nTotal number of unique parameters: {total_params_normalized:,}\")"
   ],
   "id": "2e3a6fa2797a1f60",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of unique parameters: 596,049,920\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:22:53.837014Z",
     "start_time": "2025-12-22T20:22:53.792228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def model_memory_size(model, input_dtype=torch.float32):\n",
    "    total_params = 0\n",
    "    total_grads = 0\n",
    "    for param in model.parameters():\n",
    "        # Calculate total number of elements per parameter\n",
    "        param_size = param.numel()\n",
    "        total_params += param_size\n",
    "        # Check if gradients are stored for this parameter\n",
    "        if param.requires_grad:\n",
    "            total_grads += param_size\n",
    "\n",
    "    # Calculate buffer size (non-parameters that require memory)\n",
    "    total_buffers = sum(buf.numel() for buf in model.buffers())\n",
    "\n",
    "    # Size in bytes = (Number of elements) * (Size of each element in bytes)\n",
    "    # We assume parameters and gradients are stored in the same type as input dtype\n",
    "    element_size = torch.tensor(0, dtype=input_dtype).element_size()\n",
    "    total_memory_bytes = (total_params + total_grads + total_buffers) * element_size\n",
    "\n",
    "    # Convert bytes to gigabytes\n",
    "    total_memory_gb = total_memory_bytes / (1024**3)\n",
    "\n",
    "    return total_memory_gb\n",
    "\n",
    "print(f\"float32 (PyTorch default): {model_memory_size(model, input_dtype=torch.float32):.2f} GB\")\n",
    "print(f\"bfloat16: {model_memory_size(model, input_dtype=torch.bfloat16):.2f} GB\")"
   ],
   "id": "f1b6c6e22ba3263d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 (PyTorch default): 5.64 GB\n",
      "bfloat16: 2.82 GB\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:23:44.986712Z",
     "start_time": "2025-12-22T20:23:44.740693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model.to(device);"
   ],
   "id": "66b81db583d0f88d",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:27:17.541423Z",
     "start_time": "2025-12-22T20:27:17.528618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_weights_into_qwen(model, param_config, params):\n",
    "    def assign(left, right, tensor_name=\"unknown\"):\n",
    "        if left.shape != right.shape:\n",
    "            raise ValueError(f\"Shape mismatch in tensor '{tensor_name}'. Left: {left.shape}, Right: {right.shape}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if isinstance(right, torch.Tensor):\n",
    "                left.copy_(right)\n",
    "            else:\n",
    "                left.copy_(torch.as_tensor(right, dtype=left.dtype, device=left.device))\n",
    "\n",
    "        return left\n",
    "\n",
    "    model.tok_emb.weight = assign(model.tok_emb.weight, params[\"model.embed_tokens.weight\"], \"model.embed_tokens.weight\")\n",
    "    for l in range(param_config[\"n_layers\"]):\n",
    "        block = model.trf_blocks[l]\n",
    "        att = block.att\n",
    "\n",
    "        # Q, K, V projections\n",
    "        att.W_query.weight = assign(\n",
    "            att.W_query.weight,\n",
    "            params[f\"model.layers.{l}.self_attn.q_proj.weight\"],\n",
    "            f\"model.layers.{l}.self_attn.q_proj.weight\"\n",
    "        )\n",
    "        att.W_key.weight = assign(\n",
    "            att.W_key.weight,\n",
    "            params[f\"model.layers.{l}.self_attn.k_proj.weight\"],\n",
    "            f\"model.layers.{l}.self_attn.k_proj.weight\"\n",
    "        )\n",
    "        att.W_value.weight = assign(\n",
    "            att.W_value.weight,\n",
    "            params[f\"model.layers.{l}.self_attn.v_proj.weight\"],\n",
    "            f\"model.layers.{l}.self_attn.v_proj.weight\"\n",
    "        )\n",
    "\n",
    "        # Output projection\n",
    "        att.out_proj.weight = assign(\n",
    "            att.out_proj.weight,\n",
    "            params[f\"model.layers.{l}.self_attn.o_proj.weight\"],\n",
    "            f\"model.layers.{l}.self_attn.o_proj.weight\"\n",
    "        )\n",
    "\n",
    "        # QK norms\n",
    "        if hasattr(att, \"q_norm\") and att.q_norm is not None:\n",
    "            att.q_norm.scale = assign(\n",
    "                att.q_norm.scale,\n",
    "                params[f\"model.layers.{l}.self_attn.q_norm.weight\"],\n",
    "                f\"model.layers.{l}.self_attn.q_norm.weight\"\n",
    "            )\n",
    "        if hasattr(att, \"k_norm\") and att.k_norm is not None:\n",
    "            att.k_norm.scale = assign(\n",
    "                att.k_norm.scale,\n",
    "                params[f\"model.layers.{l}.self_attn.k_norm.weight\"],\n",
    "                f\"model.layers.{l}.self_attn.k_norm.weight\"\n",
    "            )\n",
    "\n",
    "        # Attention layernorm\n",
    "        block.norm1.scale = assign(\n",
    "            block.norm1.scale,\n",
    "            params[f\"model.layers.{l}.input_layernorm.weight\"],\n",
    "            f\"model.layers.{l}.input_layernorm.weight\"\n",
    "        )\n",
    "\n",
    "        # Feedforward weights\n",
    "        block.ff.fc1.weight = assign(\n",
    "            block.ff.fc1.weight,\n",
    "            params[f\"model.layers.{l}.mlp.gate_proj.weight\"],\n",
    "            f\"model.layers.{l}.mlp.gate_proj.weight\"\n",
    "        )\n",
    "        block.ff.fc2.weight = assign(\n",
    "            block.ff.fc2.weight,\n",
    "            params[f\"model.layers.{l}.mlp.up_proj.weight\"],\n",
    "            f\"model.layers.{l}.mlp.up_proj.weight\"\n",
    "        )\n",
    "        block.ff.fc3.weight = assign(\n",
    "            block.ff.fc3.weight,\n",
    "            params[f\"model.layers.{l}.mlp.down_proj.weight\"],\n",
    "            f\"model.layers.{l}.mlp.down_proj.weight\"\n",
    "        )\n",
    "        block.norm2.scale = assign(\n",
    "            block.norm2.scale,\n",
    "            params[f\"model.layers.{l}.post_attention_layernorm.weight\"],\n",
    "            f\"model.layers.{l}.post_attention_layernorm.weight\"\n",
    "        )\n",
    "\n",
    "    # Final normalization and output head\n",
    "    model.final_norm.scale = assign(model.final_norm.scale, params[\"model.norm.weight\"], \"model.norm.weight\")\n",
    "\n",
    "    if \"lm_head.weight\" in params:\n",
    "        model.out_head.weight = assign(model.out_head.weight, params[\"lm_head.weight\"], \"lm_head.weight\")\n",
    "    else:\n",
    "        model.out_head.weight = model.tok_emb.weight\n",
    "        print(\"Model uses weight tying.\")"
   ],
   "id": "f8abc03b3792a7dc",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:28:12.713447Z",
     "start_time": "2025-12-22T20:28:12.704800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from safetensors.torch import load_file\n",
    "from huggingface_hub import hf_hub_download, snapshot_download"
   ],
   "id": "3cc1be126f4a090d",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:33:13.246366Z",
     "start_time": "2025-12-22T20:31:21.662084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if USE_REASONING_MODEL or USE_INSTRUCT_MODEL:\n",
    "    repo_id = f\"Qwen/Qwen3-{CHOOSE_MODEL}\"\n",
    "else:\n",
    "    repo_id = f\"Qwen/Qwen3-{CHOOSE_MODEL}-Base\"\n",
    "\n",
    "local_dir = Path(repo_id).parts[-1]\n",
    "\n",
    "if CHOOSE_MODEL == \"0.6B\":\n",
    "    weights_file = hf_hub_download(\n",
    "        repo_id=repo_id,\n",
    "        filename=\"model.safetensors\",\n",
    "        local_dir=local_dir,\n",
    "        token=\"<hf token>\",\n",
    "    )\n",
    "    weights_dict = load_file(weights_file)\n",
    "else:\n",
    "    repo_dir = snapshot_download(repo_id=repo_id, local_dir=local_dir)\n",
    "    index_path = os.path.join(repo_dir, \"model.safetensors.index.json\")\n",
    "    with open(index_path, \"r\") as f:\n",
    "        index = json.load(f)\n",
    "\n",
    "    weights_dict = {}\n",
    "    for filename in set(index[\"weight_map\"].values()):\n",
    "        shard_path = os.path.join(repo_dir, filename)\n",
    "        shard = load_file(shard_path)\n",
    "        weights_dict.update(shard)\n",
    "\n",
    "load_weights_into_qwen(model, QWEN3_CONFIG, weights_dict)\n",
    "model.to(device)\n",
    "del weights_dict"
   ],
   "id": "789be86730f2784f",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:45:25.770973Z",
     "start_time": "2025-12-22T20:45:25.741357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "class Qwen3Tokenizer:\n",
    "    _SPECIALS = [\n",
    "        \"<|endoftext|>\",\n",
    "        \"<|im_start|>\", \"<|im_end|>\",\n",
    "        \"<|object_ref_start|>\", \"<|object_ref_end|>\",\n",
    "        \"<|box_start|>\", \"<|box_end|>\",\n",
    "        \"<|quad_start|>\", \"<|quad_end|>\",\n",
    "        \"<|vision_start|>\", \"<|vision_end|>\",\n",
    "        \"<|vision_pad|>\", \"<|image_pad|>\", \"<|video_pad|>\",\n",
    "        \"<think>\", \"</think>\"\n",
    "    ]\n",
    "    _SPLIT_RE = re.compile(r\"(<\\|[^>]+?\\|>|<think>|</think>)\")\n",
    "\n",
    "    def __init__(self, tokenizer_file_path=\"tokenizer.json\", repo_id=None,\n",
    "                 apply_chat_template=True, add_generation_prompt=False, add_thinking=False):\n",
    "\n",
    "        self.apply_chat_template = apply_chat_template\n",
    "        self.add_generation_prompt = add_generation_prompt\n",
    "        self.add_thinking = add_thinking\n",
    "\n",
    "        tok_file = Path(tokenizer_file_path)\n",
    "        self._tok = Tokenizer.from_file(str(tok_file))\n",
    "        self._special_to_id = {}\n",
    "        for t in self._SPECIALS:\n",
    "            tid = self._tok.token_to_id(t)\n",
    "            if tid is not None:\n",
    "                self._special_to_id[t] = tid\n",
    "\n",
    "        self.pad_token_id = self._special_to_id[\"<|endoftext|>\"]\n",
    "        self.eos_token_id = self.pad_token_id\n",
    "\n",
    "        if repo_id and \"Base\" not in repo_id:\n",
    "            eos_token = \"<|im_end|>\"\n",
    "        else:\n",
    "            eos_token = \"<|endoftext|>\"\n",
    "        if eos_token in self._special_to_id:\n",
    "            self.eos_token_id = self._special_to_id[eos_token]\n",
    "\n",
    "    def encode(self, text, chat_wrapped=None):\n",
    "        if chat_wrapped is None:\n",
    "            chat_wrapped = self.apply_chat_template\n",
    "\n",
    "        stripped = text.strip()\n",
    "        if stripped in self._special_to_id and \"\\n\" not in stripped:\n",
    "            return [self._special_to_id[stripped]]\n",
    "\n",
    "        if chat_wrapped:\n",
    "            text = self._wrap_chat(text)\n",
    "\n",
    "        ids = []\n",
    "        for part in filter(None, self._SPLIT_RE.split(text)):\n",
    "            if part in self._special_to_id:\n",
    "                ids.append(self._special_to_id[part])\n",
    "            else:\n",
    "                ids.extend(self._tok.encode(part).ids)\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return self._tok.decode(ids, skip_special_tokens=False)\n",
    "\n",
    "    def _wrap_chat(self, user_msg):\n",
    "        s = f\"<|im_start|>user\\n{user_msg}<|im_end|>\\n\"\n",
    "        if self.add_generation_prompt:\n",
    "            s += \"<|im_start|>assistant\"\n",
    "            if self.add_thinking:\n",
    "                s += \"\\n\"\n",
    "            else:\n",
    "                s += \"\\n<think>\\n\\n</think>\\n\\n\"\n",
    "        return s"
   ],
   "id": "95a3b092886c783e",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:45:35.897475Z",
     "start_time": "2025-12-22T20:45:31.527268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if USE_REASONING_MODEL:\n",
    "    tokenizer_file_path = f\"Qwen3-{CHOOSE_MODEL}/tokenizer.json\"\n",
    "else:\n",
    "    tokenizer_file_path = f\"Qwen3-{CHOOSE_MODEL}-Base/tokenizer.json\"\n",
    "\n",
    "hf_hub_download(\n",
    "    repo_id=repo_id,\n",
    "    filename=\"tokenizer.json\",\n",
    "    local_dir=local_dir,\n",
    ")\n",
    "\n",
    "if USE_REASONING_MODEL or USE_INSTRUCT_MODEL:\n",
    "    tokenizer = Qwen3Tokenizer(\n",
    "        tokenizer_file_path=tokenizer_file_path,\n",
    "        repo_id=repo_id,\n",
    "        apply_chat_template=True,\n",
    "        add_generation_prompt=True,\n",
    "        add_thinking=USE_REASONING_MODEL\n",
    "    )\n",
    "\n",
    "else:\n",
    "    tokenizer = Qwen3Tokenizer(\n",
    "        tokenizer_file_path=tokenizer_file_path,\n",
    "        repo_id=repo_id,\n",
    "        apply_chat_template=False,\n",
    "        add_generation_prompt=False,\n",
    "        add_thinking=False\n",
    "    )"
   ],
   "id": "1b7c3633a45561a",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:47:13.435418Z",
     "start_time": "2025-12-22T20:47:00.004649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"Give me a short introduction to large language models.\"\n",
    "\n",
    "input_token_ids = tokenizer.encode(prompt)\n",
    "text = tokenizer.decode(input_token_ids)\n",
    "text"
   ],
   "id": "683e886414769535",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\nGive me a short introduction to large language models.<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:47:26.552880Z",
     "start_time": "2025-12-22T20:47:26.540280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_text_basic_stream(model, token_ids, max_new_tokens, eos_token_id=None):\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            out = model(token_ids)[:, -1]\n",
    "            next_token = torch.argmax(out, dim=-1, keepdim=True)\n",
    "\n",
    "            if (eos_token_id is not None\n",
    "                   and torch.all(next_token == eos_token_id)):\n",
    "               break\n",
    "\n",
    "            yield next_token\n",
    "\n",
    "            token_ids = torch.cat([token_ids, next_token], dim=1)"
   ],
   "id": "e2558d7f17bd4f09",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-12-22T20:52:29.236329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "p = \"can you explain event horizon ?\"\n",
    "input_token_ids = tokenizer.encode(p)\n",
    "input_token_ids_tensor = torch.tensor(input_token_ids, device=device).unsqueeze(0)\n",
    "\n",
    "\n",
    "for token in generate_text_basic_stream(\n",
    "    model=model,\n",
    "    token_ids=input_token_ids_tensor,\n",
    "    max_new_tokens=500,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    "):\n",
    "    token_id = token.squeeze(0).tolist()\n",
    "    print(\n",
    "        tokenizer.decode(token_id),\n",
    "        end=\"\",\n",
    "        flush=True\n",
    "    )"
   ],
   "id": "57246eeac1876891",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user asked for an explanation of the event horizon. Let me start by recalling what I know. The event horizon is a concept in general relativity, right? It's the boundary of a black hole. So, the user probably wants a basic definition and maybe some key points.\n",
      "\n",
      "First, I should mention that the event horizon is the point where time and space wrap around the black hole. Then, there's the singularity at the center, which is a point with"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a09d5362515a51a1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
