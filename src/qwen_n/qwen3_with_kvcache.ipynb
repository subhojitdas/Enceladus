{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-28T15:43:44.911592Z",
     "start_time": "2025-12-28T15:43:43.971839Z"
    }
   },
   "source": [
    "import torch\n",
    "from qwen3_model_with_kvcache import Qwen3ModelWithKVCache\n",
    "from qwen3_tok import Qwen3Tokenizer, load_tokenizer\n",
    "from qwen3_model_with_kvcache import load_weight\n",
    "from qwen3_model_with_kvcache import KVCache\n",
    "import time"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T15:43:45.621500Z",
     "start_time": "2025-12-28T15:43:45.611358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"huggingface_hub\",  # to download pretrained weights\n",
    "    \"tokenizers\",       # to implement the tokenizer\n",
    "    \"torch\",            # to implement the model\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ],
   "id": "a35616da7b28f2d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface_hub version: 1.2.3\n",
      "tokenizers version: 0.22.1\n",
      "torch version: 2.9.1\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T15:43:46.407157Z",
     "start_time": "2025-12-28T15:43:46.401342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CHOOSE_MODEL = \"0.6B\"\n",
    "\n",
    "QWEN3_CONFIG = {\n",
    "        \"vocab_size\": 151_936,           # Vocabulary size\n",
    "        \"context_length\": 40_960,        # Context length that was used to train the model\n",
    "        \"emb_dim\": 1024,                 # Embedding dimension\n",
    "        \"n_heads\": 16,                   # Number of attention heads\n",
    "        \"n_layers\": 28,                  # Number of layers\n",
    "        \"hidden_dim\": 3072,              # Size of the intermediate dimension in FeedForward\n",
    "        \"head_dim\": 128,                 # Size of the heads in GQA\n",
    "        \"qk_norm\": True,                 # Whether to normalize queries and keys in GQA\n",
    "        \"n_kv_groups\": 8,                # Key-Value groups for grouped-query attention\n",
    "        \"rope_base\": 1_000_000.0,        # The base in RoPE's \"theta\"\n",
    "        \"dtype\": torch.bfloat16,         # Lower-precision dtype to reduce memory usage\n",
    "    }"
   ],
   "id": "d2a306b7296565e4",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T15:43:49.618537Z",
     "start_time": "2025-12-28T15:43:47.092008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(18)\n",
    "model = Qwen3ModelWithKVCache(QWEN3_CONFIG)"
   ],
   "id": "f771b1ffc0ac2fc1",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T15:43:51.574268Z",
     "start_time": "2025-12-28T15:43:50.801894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model.to(device)"
   ],
   "id": "989387c099be7370",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ModelWithKVCache(\n",
       "  (tok_emb): Embedding(151936, 1024)\n",
       "  (trf_blocks): ModuleList(\n",
       "    (0-27): 28 x TransformerBlock(\n",
       "      (att): GroupedQueryAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        (q_norm): RMSNorm()\n",
       "        (k_norm): RMSNorm()\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (fc2): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (fc3): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "      )\n",
       "      (norm1): RMSNorm()\n",
       "      (norm2): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (final_norm): RMSNorm()\n",
       "  (out_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T15:43:54.380248Z",
     "start_time": "2025-12-28T15:43:52.738584Z"
    }
   },
   "cell_type": "code",
   "source": "load_weight(model, device, QWEN3_CONFIG, CHOOSE_MODEL)",
   "id": "a70faefb19c65f64",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T15:43:56.349118Z",
     "start_time": "2025-12-28T15:43:55.956147Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer = load_tokenizer(CHOOSE_MODEL)",
   "id": "f5f2dc24da83dbda",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T15:43:57.275598Z",
     "start_time": "2025-12-28T15:43:57.263172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_text_basic_stream(model, token_ids, max_new_tokens, eos_token_id=None, context_size=None):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        cache = KVCache(n_layers=model.cfg[\"n_layers\"])\n",
    "        model.reset_kv_cache()\n",
    "\n",
    "        # Prime the cache with the initial context\n",
    "        logits = model(token_ids, cache=cache)\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            next_token = torch.argmax(logits[:, -1], dim=-1, keepdim=True)\n",
    "\n",
    "            if eos_token_id is not None and torch.all(next_token == eos_token_id):\n",
    "                break\n",
    "\n",
    "            yield next_token\n",
    "\n",
    "            token_ids = torch.cat([token_ids, next_token], dim=1)\n",
    "\n",
    "            # Feed only the new token to the model; cache handles history\n",
    "            logits = model(next_token, cache=cache)"
   ],
   "id": "5973d9047c1af2a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T15:45:26.649176Z",
     "start_time": "2025-12-28T15:45:09.372827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "p = \"Can you explain what MultiHead latent attention ?\"\n",
    "input_token_ids = tokenizer.encode(p)\n",
    "input_token_ids_tensor = torch.tensor(input_token_ids, device=device).unsqueeze(0)\n",
    "\n",
    "gen_tokens = 0\n",
    "t0 = time.time()\n",
    "\n",
    "for token in generate_text_basic_stream(\n",
    "    model=model,\n",
    "    token_ids=input_token_ids_tensor,\n",
    "    max_new_tokens=8192,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    "):\n",
    "    token_id = token.squeeze(0).tolist()\n",
    "    gen_tokens += 1\n",
    "    text = tokenizer.decode(token_id)\n",
    "    print(\n",
    "        tokenizer.decode(token_id),\n",
    "        end=\"\",\n",
    "        flush=True\n",
    "    )\n",
    "\n",
    "decode_time = time.time() - t0\n",
    "decode_toks_per_sec = gen_tokens / decode_time\n",
    "\n",
    "print(\"\\n\\ntoken/s\", decode_toks_per_sec)"
   ],
   "id": "f56fa695ccc2ead8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to explain what MultiHead Latent Attention is. Hmm, I remember that in neural networks, especially in transformer models, attention mechanisms are used to find the most relevant parts of the input. But how does MultiHead work?\n",
      "\n",
      "Wait, MultiHead is a technique where the attention mechanism has multiple heads. Each head is a separate attention mechanism. So maybe each head is responsible for different parts of the input. For example, one head might focus on the first part, another on the second, and so on. That way, the model can capture different aspects of the input.\n",
      "\n",
      "But then there's the latent attention part. Oh right, latent attention is a variation of the attention mechanism where the input is represented as a latent space. So instead of looking at the actual input, the model is looking at the latent representation. This might help in capturing more abstract or higher-level features.\n",
      "\n",
      "So putting it all together, MultiHead Latent Attention is a method where the attention mechanism uses multiple heads to process the input in different ways, and the input is represented in a latent space. This allows the model to capture more complex relationships between the input features.\n",
      "\n",
      "Wait, but I should make sure I'm not mixing up the terms. Latent space usually refers to a lower-dimensional representation, while attention is about finding the relevant parts. So maybe the latent attention is a way to represent the input in a latent space and then use attention to find the relevant parts. That makes sense.\n",
      "\n",
      "I think that's it. So the key points are: MultiHead is the technique with multiple heads, and latent attention is the method where the input is represented in a latent space and attention is used to find relevant parts.\n",
      "</think>\n",
      "\n",
      "**MultiHead Latent Attention** is a technique used in transformer-based models to enhance the ability of the model to capture complex relationships between input features. Here's a breakdown:\n",
      "\n",
      "1. **MultiHead Attention**:  \n",
      "   - This is a core component of the attention mechanism. It uses **multiple heads** (each a separate attention mechanism) to process the input. Each head is responsible for focusing on different parts of the input, allowing the model to capture diverse representations.  \n",
      "   - For example, one head might emphasize the first part of the input, another the second, and so on, enabling the model to extract different features.\n",
      "\n",
      "2. **Latent Attention**:  \n",
      "   - This is a variation where the input is represented in a **latent space** (a lower-dimensional representation) instead of the original input.  \n",
      "   - The model uses the latent representation to find relevant features, which can help in abstracting or capturing higher-level patterns. This approach often improves performance by reducing dimensionality and enabling more efficient feature extraction.\n",
      "\n",
      "3. **Combined Effect**:  \n",
      "   - By combining **multi-head attention** with **latent attention**, the model can effectively handle complex relationships and abstract features, leading to better performance in tasks like language modeling or sequence prediction.\n",
      "\n",
      "In summary, **MultiHead Latent Attention** leverages the power of multiple attention heads and latent space representation to capture nuanced and abstract features from the input.\n",
      "\n",
      "token/s 36.55621418568325\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "32b3b68191bbfc14"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
